---
title: "FinalProject"
author: "Apu Chandra Das"
date: "6/13/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The original training and testing data consist of 160 variables. At first, as a part of data processing I removed columns with NA entries, which brought the number of variables down to 60. Then I removed 6 additional variables which contained information that I don't think helpful for our analysis: X, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, num_window. (Prior to removing these variables, I was achieving perfect accuracy on my training and validation sets, but my model was predicting all of the test cases to be of classe A.) Aside from user_name and classe/problem_id, all of the remaining variables appear to be measurements from the fitness device.

I did split the training data into two sets: "trainingdata2" for training the model (60%) and "validation" for checking the validation of the model (40%). I set the seed 111 for the purpose of reproducibility. I performed a random forest model on "trainingdata2" using the default parameters and library caret. I prefer random forest model as they tend to be very accurate  and the data set was small enough.

The outcome variable classes on "traindata2" was predicted and found the accuracy to be 100%. THen using this model I predicted te outcome of "validation" data set and found the accuracy to be almost 100%. Since this was the first model that I tried (as opposed to trying multiple models and selecting the best performer on the validation set), I expect the out of sample error to be around 99%.

FInally, I applied this model to the testing data set, and it correctly identified all 20 cases.



```{r, warning=FALSE}
setwd("C:/Users/apust/Desktop/Machine Learning/Practical Machine Learning JHU")
library(caret)
library(randomForest)

# read both training and testing data set  and identify the NA's
traindata <- read.csv("pml-training.csv", na.strings = c("NA", "#DIV/0!", ""))
testdata  <- read.csv("pml-testing.csv", na.strings = c("NA", "#DIV/0!", ""))

# remove NA columns for both training and testing data
comps <- complete.cases(t(traindata)) & complete.cases(t(testdata))
traindata <- traindata[,comps]
testdata  <- testdata[,comps]

# remove predictors that don't seem useful
traindata <- traindata[,-c(1,3,4,5,6,7)]
testdata <- testdata[,-c(1,3,4,5,6,7)]

# splitting data
set.seed(111)
inTrain <- createDataPartition(traindata$classe, p=0.6, list=FALSE)
traindata2 <- traindata[inTrain,]
validation <- traindata[-inTrain,]

# fitting a model
modFit <- randomForest(classe~., data=traindata2)

# the results on the training set
trainresults <- predict(modFit, traindata2)
trainacc <- sum(trainresults==traindata2$classe)/length(trainresults)
paste("Accuracy on training set =",trainacc)

# the results on the validation set
validresults <- predict(modFit, newdata=validation)
validacc <- sum(validresults==validation$classe)/length(validresults)
paste("Accuracy on validation set =",validacc)

# the results on the test set
testresults <- predict(modFit, newdata=testdata)
print("Classifications on the test set:"); testresults
```
